{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection: Descriptive Statistics\n",
    "* Descriptives of data collected during training and testing phase.\n",
    "* Preparation of testing phase data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "import ast\n",
    "import re\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WebSearcher as ws\n",
    "ws.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"immigration/backup/\"\n",
    "out_path = \"immigration/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data collection descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(PATH+\"data_test.txt\", sep=\"\\t\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec2ins = pd.read_csv(PATH+\"recipe2instance.txt\", sep=\"\\t\")\n",
    "rec2ins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = pd.read_csv(PATH+\"recipe.txt\", sep=\"\\t\", lineterminator=\"\\n\")#, engine=\"python\")\n",
    "recipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = pd.read_csv(PATH+\"run.txt\", sep=\"\\t\")\n",
    "run = run.rename(columns={'uid': 'run_uid'})\n",
    "run.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipestep = pd.read_csv(PATH+\"recipestep.txt\", sep=\"\\t\")\n",
    "html_steps = recipestep[recipestep.value=='innerHTML'].uid.to_list()\n",
    "autocomplete_steps = recipestep[recipestep.type=='get_texts'].uid.to_list()\n",
    "recipestep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_round(name):\n",
    "    parts = name.split(\"_\")\n",
    "    test_num = parts[0].strip('test')\n",
    "    round_num = parts[1]\n",
    "    #print(test_num, round_num)\n",
    "    return int(test_num), int(round_num)\n",
    "\n",
    "def get_group(name):\n",
    "    parts = name.split(\"_\")\n",
    "    if name.startswith('test'):\n",
    "        group_num = parts[2]\n",
    "    else:\n",
    "        group_num = parts[1]\n",
    "    return group_num\n",
    "\n",
    "def get_conditions(name):\n",
    "    parts = name.split('_')\n",
    "    if name.startswith('train'):\n",
    "        search_history = parts[3]\n",
    "        user_input = parts[4][:-4]\n",
    "        return search_history, user_input\n",
    "    elif name.startswith('test'):\n",
    "        search_history = parts[4]\n",
    "        user_input = parts[5][:-4]\n",
    "        return search_history, user_input\n",
    "    else:\n",
    "        return 'unknown', 'unknown'\n",
    "\n",
    "def get_train_test(name):\n",
    "    if name.startswith('test'):\n",
    "        return 'test'\n",
    "    elif name.startswith('train'):\n",
    "        return 'train'\n",
    "    else:\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec2ins[['test_num', 'round_num']] = rec2ins[rec2ins.new_name.str.startswith('test')]['new_name'].apply(lambda x: pd.Series(get_test_round(x)))\n",
    "rec2ins['group'] = rec2ins['new_name'].apply(get_group)\n",
    "rec2ins['train_test'] = rec2ins['new_name'].apply(get_train_test)\n",
    "rec2ins[['search_history', 'user_input']] = rec2ins['new_name'].apply(lambda x: pd.Series(get_conditions(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [c for c in rec2ins.columns if c not in [\"created\", \"uid\"]]\n",
    "run = pd.merge(run, rec2ins[keep_cols], how='left', on=['recipe_uid', 'instance_uid'])\n",
    "run.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First round of data collection accidentally collected some agents multiple times (see logbook immigration). \n",
    "#Therefore, select first test row for every agent.\n",
    "test_run = run[run.train_test=='test'].sort_values('created').drop_duplicates(['new_name'], keep='first')\n",
    "train_run = run[run['train_test'] != 'test']\n",
    "run = pd.concat([test_run, train_run])\n",
    "\n",
    "print('Number of runs per round:')\n",
    "run[run.train_test=='test'].round_num.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of runs:', len(run))\n",
    "run.train_test.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each agent trained on average how many times?\n",
    "print('Average number of training runs per agent:')\n",
    "len(run[run.train_test=='train']) / run[run.train_test=='train'].new_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of successful/unsuccessful runs:')\n",
    "run.groupby('train_test').status.value_counts(dropna=False)\n",
    "# 24 errors in training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of (un)successful runs per group:')\n",
    "run.groupby(['group'])['status'].value_counts(dropna=False)\n",
    "# does not occur only in one group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run['created'] = pd.to_datetime(run.created)\n",
    "print('Training data collected between', run[run.train_test=='train'].created.min(), 'and', run[run.train_test=='train'].created.max())\n",
    "print('Test data collected between', run[run.train_test=='test'].created.min(), 'and', run[run.train_test=='test'].created.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmin = run[run.round_num==1].created.min()\n",
    "dmax = run[run.round_num==1].created.max()\n",
    "print('Test data round 1 collected between', dmin, 'and', dmax, ', Timedelta:', dmax-dmin)\n",
    "dmin = run[run.round_num==2].created.min()\n",
    "dmax = run[run.round_num==2].created.max()\n",
    "print('Test data round 2 collected between', dmin, 'and', dmax, ', Timedelta:', dmax-dmin)\n",
    "dmin = run[run.round_num==3].created.min()\n",
    "dmax = run[run.round_num==3].created.max()\n",
    "print('Test data round 3 collected between', dmin, 'and', dmax, ', Timedelta:', dmax-dmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, run.drop('created' ,axis=1), on='run_uid', how='left')\n",
    "data = data[~data.new_name.isna()] # drop those removed first round data collections \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[(data.value!=np.nan)&(data.value.str.startswith(\"<\")),'html'] = 1\n",
    "html = data[data.step_uid.isin(html_steps)]\n",
    "print('HTML unsuccessfully collected in', html.value.isna().sum(), 'cases.')\n",
    "print(len(html), 'htmls collected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_dct = {1:\n",
    "       {'pro':'vluchtelingen',\n",
    "       'anti':'asielzoekers',\n",
    "       'neutral':'immigratie'},\n",
    "       2:\n",
    "       {'pro':'vluchtelingencrisis',\n",
    "       'anti':'azc',\n",
    "       'neutral':'imigranten'},\n",
    "       3:\n",
    "       {'pro':'vluchtelingenproblematiek',\n",
    "       'anti':'criminaliteit onder asielzoekers',\n",
    "       'neutral':'immigratiecijfers'}\n",
    "      }\n",
    "\n",
    "def clean_up_list(lst, test_num, user_input):\n",
    "    #print(lst, test_num, user_input)\n",
    "    remove = sq_dct[test_num][user_input]\n",
    "    lst = [w for w in lst if (not pd.isna(w)) & (w != remove)]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocompletes - remove if missing AND if same as input query\n",
    "autocompletes = data[data.step_uid.isin(autocomplete_steps)]\n",
    "autocompletes = autocompletes.groupby('run_uid')['value'].agg(list).reset_index()\n",
    "autocompletes = pd.merge(autocompletes, data.drop_duplicates('run_uid')[['run_uid', 'test_num', 'round_num', 'group', 'train_test','search_history', 'user_input']], on='run_uid', how='left')\n",
    "autocompletes['autocompletes'] = autocompletes.apply(lambda x: clean_up_list(x['value'], x['test_num'], x['user_input']), axis=1)\n",
    "autocompletes.drop(columns=[\"value\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocompletes['ac_length'] = autocompletes['autocompletes'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write files\n",
    "html.to_csv(out_path+\"html_test.csv\", index=False)\n",
    "autocompletes.to_csv(out_path+\"autocompletes_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Parsing of SERPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parsing SERP HTMLs\n",
    "def parse_serp(run_uid, value):\n",
    "    #soup = ws.load_soup(html)\n",
    "    soup = ws.make_soup(value)\n",
    "    parsed = ws.parse_serp(soup)\n",
    "    results = pd.DataFrame(parsed)\n",
    "    \n",
    "    if \"sub_type\" in results.columns: # not present in all parsed SERPs\n",
    "        # make type distinction between two knowledge types: panel_rhs (knowledge panel) and featured snippet\n",
    "        results.loc[results['type'] == 'knowledge', 'type'] = results['type'] + \"_\" + results['sub_type']\n",
    "    \n",
    "    # knowledge panel = rank -1\n",
    "    results.loc[results['type'] == 'knowledge_panel_rhs', 'serp_rank'] = -1\n",
    "    results.loc[results['type'] == 'knowledge_panel_rhs', 'cmpt_rank'] = -1\n",
    "\n",
    "    # twitter cards: # header + after third card not visible on SERP\n",
    "    mask = (results['type']=='twitter_cards')&((results['sub_type'] == 'header')|(results['sub_rank'] >= 4))\n",
    "    results.loc[mask, 'not_visible'] = 1\n",
    "    \n",
    "    # videos: # after third not visible on SERP\n",
    "    mask = (results['type']=='videos')&(results['sub_rank'] >= 3)\n",
    "    results.loc[mask, 'not_visible'] = 1\n",
    "    \n",
    "    # add identifier\n",
    "    results['run_uid'] = run_uid\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = html.iloc[0:100]\n",
    "#test_results = []\n",
    "#for run_uid, value in tqdm(zip(test['run_uid'], test['value'])):\n",
    "#    #print(run_uid)\n",
    "#    res = parse_serp(run_uid, value)\n",
    "#    test_results.append(res)\n",
    "#test_results = pd.concat(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for run_uid, value in tqdm(zip(html['run_uid'], html['value'])):\n",
    "    res = parse_serp(run_uid, value)\n",
    "    results.append(res)\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only rows that have reasonably been shown on the SERP.\n",
    "# some unknown ones, e.g., \"Resultaten zoeken voor...\", \"Vergelijkingssites\"\n",
    "print(results.type.value_counts())\n",
    "results = results[results['not_visible']!=1].copy()\n",
    "print(results.type.value_counts(), results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce images to one row each.\n",
    "images_rows = results[results['type']=='images'].groupby('run_uid').first().reset_index()\n",
    "results = pd.concat([images_rows, results[results['type']!='images']])\n",
    "results = results.sort_values(['run_uid', 'serp_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subrank rows for people_also_ask and searches_related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_people_also_ask(row):\n",
    "    details_list = row['details']\n",
    "    new_rows=[]\n",
    "    i=0\n",
    "    for d in details_list:\n",
    "        new_row = {'run_uid':row['run_uid'], 'type': row['type'], 'text': d, 'sub_rank': i, 'cmpt_rank':row['cmpt_rank'], 'serp_rank': row['serp_rank']}\n",
    "        new_rows.append(new_row)\n",
    "        i+=1\n",
    "    return new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "people_also_ask_rows = results[results['type']=='people_also_ask']\n",
    "expanded_rows = pd.DataFrame(people_also_ask_rows.apply(expand_people_also_ask, axis=1).sum())\n",
    "results = pd.concat([expanded_rows, results[results['type']!='people_also_ask']])\n",
    "results = results.sort_values(['run_uid', 'serp_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_searches_related(row):\n",
    "    details_list = row['details']\n",
    "    new_rows=[]\n",
    "    i=0\n",
    "    for d in details_list:\n",
    "        text = d['text']\n",
    "        new_row = {'run_uid':row['run_uid'], 'type': row['type'], 'text': text, 'sub_rank': i, 'cmpt_rank':row['cmpt_rank'], 'serp_rank': row['serp_rank']}\n",
    "        new_rows.append(new_row)\n",
    "        i+=1\n",
    "    return new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches_related_rows = results[results['type']=='searches_related']\n",
    "expanded_rows = pd.DataFrame(searches_related_rows.apply(expand_searches_related, axis=1).sum())\n",
    "results = pd.concat([expanded_rows, results[results['type']!='searches_related']])\n",
    "results = results.sort_values(['run_uid', 'serp_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippet_details(row):\n",
    "    details = row['details']\n",
    "    \n",
    "    row['url'] = details['urls'][0]['url'].strip('.')\n",
    "    row['title'] = details['heading']\n",
    "    row['text'] = details['text']\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results['type'] == 'knowledge_featured_snippet'] = results.loc[results['type'] == 'knowledge_featured_snippet'].apply(get_snippet_details,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_domain(url):\n",
    "    if isinstance(url, str):\n",
    "        extracted = tldextract.extract(url)\n",
    "        domain = extracted.domain + \".\" + extracted.suffix\n",
    "        return domain\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse domain from URL\n",
    "results['domain'] = results['url'].apply(parse_domain)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure empty strings are NaN values\n",
    "results.loc[(results['domain']=='')|(results['domain']=='.'), 'domain'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[~results.domain.isna()].domain.unique(), results[~results.domain.isna()].domain.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add domain categories\n",
    "cats = pd.read_csv(\"news_categories_adjusted_domainlevel.csv\")\n",
    "print('Number of domains in categorisation file:', len(cats))\n",
    "results = pd.merge(results, cats[~cats.domain.isna()], on=['domain'], how='left')\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_core = missing final_category\n",
    "results.loc[(results.final_category=='non_core'), 'final_category'] = np.nan\n",
    "# fix avrotros mistake\n",
    "results.loc[(results.final_category=='special_int'), 'final_category'] = 'inst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# share of rows with domains identified\n",
    "#results[~results['domain'].isna()].final_category.value_counts(dropna=False)\n",
    "results[~results['domain'].isna()].final_category.value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#missingcat_domains = results[(~results['domain'].isna())&(results.final_category.isna())].domain.unique()\n",
    "#pd.DataFrame(missingcat_domains, columns=['domain']).to_csv('missingcat_domains_im.csv')\n",
    "#missingcat_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional coding of missing domains\n",
    "cats2 = pd.read_csv(\"finalcategory_coding.csv\", sep=\";\")\n",
    "cats2 = cats2.rename(columns={'final_category': 'final_category2'})\n",
    "cats2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.merge(results, cats2, on='domain', how='left')\n",
    "results.loc[(~results['domain'].isna())&(results.final_category.isna()), 'final_category'] = results['final_category2']\n",
    "results.drop(columns=['final_category2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% coverage\n",
    "results[(~results['domain'].isna())].final_category.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[(~results['domain'].isna())].final_category.value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_video_title(row):\n",
    "    # title is everything before the channel\n",
    "    title = row['title']\n",
    "    channel = row['cite']\n",
    "    cleaned_title = title.split(channel)[0].strip()\n",
    "    ## Remove YouTube manually\n",
    "    #cleaned_title = cleaned_title.replace('YouTube', '')\n",
    "    row['title'] = cleaned_title\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results['type'] == 'videos'] = results.loc[results['type'] == 'videos'].apply(clean_video_title, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset serp_rank (necessary given rows expanded).\n",
    "results = results.sort_values(['run_uid', 'serp_rank', 'sub_rank'])\n",
    "results['serp_rank'] = results.groupby('run_uid').cumcount() + 1\n",
    "# knowledge panel = rank -1\n",
    "results.loc[results['type'] == 'knowledge_panel_rhs', 'serp_rank'] = -1\n",
    "results.loc[results['type'] == 'knowledge_panel_rhs', 'cmpt_rank'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(out_path+\"result_data_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
